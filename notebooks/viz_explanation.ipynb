{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "root_dir = \"../\"\n",
    "sys.path.append(root_dir)\n",
    "from datasets import CUB\n",
    "from classifiers import CLIPClassifier\n",
    "from speaker_model import ClaimSpeaker\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "backbone = \"ViT-L/14\"\n",
    "classifier = CLIPClassifier(backbone, device=device)\n",
    "\n",
    "dataset = CUB(\n",
    "    data_dir, train=False, transform=classifier.preprocess, return_attribute=True\n",
    ")\n",
    "classes = dataset.classes\n",
    "claims = dataset.claims\n",
    "\n",
    "class_prompts = [f\"A photo of a {class_name}\" for class_name in classes]\n",
    "\n",
    "beta = 0.4\n",
    "speaker = ClaimSpeaker(classifier, len(classes), claims, device=device)\n",
    "speaker_state_path = os.path.join(root_dir, \"weights\", f\"speaker_10_0.0_{beta}_8.pt\")\n",
    "speaker.load_state_dict(torch.load(speaker_state_path))\n",
    "speaker.eval()\n",
    "\n",
    "pragmatic_speaker = ClaimSpeaker(classifier, len(classes), claims, device=device)\n",
    "pragmatic_speaker_state_path = os.path.join(\n",
    "    root_dir, \"weights\", f\"speaker_10_0.05_{beta}_8.pt\"\n",
    ")\n",
    "pragmatic_speaker.load_state_dict(torch.load(pragmatic_speaker_state_path))\n",
    "pragmatic_speaker.eval()\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 8\n",
    "idx = rng.choice(len(dataset), m, replace=False)\n",
    "\n",
    "for _idx in idx:\n",
    "    image, attribute = dataset[_idx]\n",
    "    image_path, _ = dataset.samples[_idx]\n",
    "\n",
    "    image = image.to(device).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        cls_output = classifier(image, class_prompts)\n",
    "        image_features = cls_output[\"image_features\"]\n",
    "        logits = cls_output[\"logits\"]\n",
    "        prediction = torch.argmax(logits).item()\n",
    "\n",
    "        prediction_name = classes[prediction]\n",
    "\n",
    "    b = 10\n",
    "    image_features = image_features.unsqueeze(1).expand(-1, 10, -1).float()\n",
    "    prediction = torch.tensor([prediction]).long().to(device)\n",
    "\n",
    "    k = 4\n",
    "    explanation, explanation_logp = speaker.explain(image_features, prediction, k)\n",
    "    explanation = explanation.squeeze()\n",
    "\n",
    "    pragmatic_explanation, pragmatic_explanation_logp = pragmatic_speaker.explain(\n",
    "        image_features, prediction, k\n",
    "    )\n",
    "    pragmatic_explanation = pragmatic_explanation.squeeze()\n",
    "\n",
    "    _, ax = plt.subplots(figsize=(3, 3))\n",
    "    image_raw = Image.open(image_path).convert(\"RGB\")\n",
    "    ax.imshow(image_raw)\n",
    "    ax.axis(\"off\")\n",
    "    ax.set_title(f\"Prediction: {prediction_name}\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Not pragmatic speaker\")\n",
    "    for _explanation in explanation:\n",
    "        explanation_claims = [claims[i] for i in _explanation if i < len(claims)]\n",
    "        explanation_gt = [attribute[i] for i in _explanation if i < len(claims)]\n",
    "        print(list(zip(explanation_claims, explanation_gt)))\n",
    "    print(\"==========\")\n",
    "\n",
    "    print(\"Pragmatic speaker\")\n",
    "    for _explanation in pragmatic_explanation:\n",
    "        explanation_claims = [claims[i] for i in _explanation if i < len(claims)]\n",
    "        explanation_gt = [attribute[i] for i in _explanation if i < len(claims)]\n",
    "        print(list(zip(explanation_claims, explanation_gt)))\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
