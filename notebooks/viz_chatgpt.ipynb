{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_fscore_support\n",
    "\n",
    "root_dir = \"../\"\n",
    "sys.path.append(root_dir)\n",
    "from datasets import SKINCON\n",
    "\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "results_dir = os.path.join(root_dir, \"results\")\n",
    "\n",
    "dataset = SKINCON(data_dir)\n",
    "attributes = dataset.attributes\n",
    "attributes = list(map(str.lower, attributes))\n",
    "\n",
    "filenames = np.array([path.split(\"/\")[-1] for path, _ in dataset.samples])\n",
    "image_attribute = np.array(dataset.image_attribute)\n",
    "\n",
    "sns.set_theme()\n",
    "sns.set_context(\"paper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"gpt-4o-mini\"\n",
    "response_path = os.path.join(\n",
    "    results_dir, f\"{model_name.replace('-', '_')}_responses.json\"\n",
    ")\n",
    "responses = json.load(open(response_path, \"r\"))\n",
    "\n",
    "results = -2 * np.ones((len(filenames), len(attributes)))\n",
    "for path, response in responses.items():\n",
    "    filename = path.split(\"/\")[-1]\n",
    "    image_mask = filenames == filename\n",
    "\n",
    "    for _, choice in response.items():\n",
    "        annotations = choice[\"annotations\"]\n",
    "\n",
    "        # labels = -2 * np.ones(len(attributes))\n",
    "        for annotation in annotations:\n",
    "            attribute = annotation[\"attribute\"]\n",
    "            attribute_label = annotation[\"label\"]\n",
    "\n",
    "            attribute_idx = attributes.index(attribute.strip())\n",
    "            results[image_mask, attribute_idx] = attribute_label\n",
    "            # labels[attribute_idx] = attribute_label\n",
    "\n",
    "    # results[image_mask] = labels\n",
    "\n",
    "n_refusals = np.sum(results == -1)\n",
    "print(f\"Number of refusals: {n_refusals} ({n_refusals/results.size:.2%})\")\n",
    "\n",
    "results[results == -1] = 0.0\n",
    "assert np.all(results != -2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr, fpr, thresholds, auroc = [], [], [], []\n",
    "for attribute_idx, attribute in enumerate(attributes):\n",
    "    attribute_score = results[:, attribute_idx]\n",
    "    ground_truth = image_attribute[:, attribute_idx]\n",
    "\n",
    "    attribute_fpr, attribute_tpr, attribute_thresholds = roc_curve(\n",
    "        ground_truth, attribute_score\n",
    "    )\n",
    "    attribute_auc = auc(attribute_fpr, attribute_tpr)\n",
    "\n",
    "    tpr.append(attribute_tpr)\n",
    "    fpr.append(attribute_fpr)\n",
    "    thresholds.append(attribute_thresholds)\n",
    "    auroc.append(attribute_auc)\n",
    "\n",
    "sorted_idx = np.argsort(auroc)[::-1]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(5, 5))\n",
    "for attribute_idx in sorted_idx:\n",
    "    attribute = attributes[attribute_idx]\n",
    "    ax.plot(\n",
    "        fpr[attribute_idx],\n",
    "        tpr[attribute_idx],\n",
    "        label=f\"{attribute} (AUC = {auroc[attribute_idx]:.2f})\",\n",
    "    )\n",
    "ax.plot([0, 1], [0, 1], \"--\", color=\"gray\", label=\"Random\")\n",
    "ax.set_title(f\"Mean AUC: {np.mean(auroc):.2f} +- {np.std(auroc):.2f}\")\n",
    "ax.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = np.linspace(0, 1, 21)\n",
    "\n",
    "precision, recall, fscore = [], [], []\n",
    "for attribute_idx, attribute in enumerate(attributes):\n",
    "    attribute = attributes[attribute_idx]\n",
    "    attribute_score = results[:, attribute_idx]\n",
    "    ground_truth = image_attribute[:, attribute_idx]\n",
    "\n",
    "    attribute_precision, attribute_recall, attribute_fscore = [], [], []\n",
    "    for t in threshold:\n",
    "        attribute_pred = attribute_score >= t\n",
    "\n",
    "        _precision, _recall, _fscore, _ = precision_recall_fscore_support(\n",
    "            ground_truth, attribute_pred, average=\"binary\", zero_division=0\n",
    "        )\n",
    "        attribute_precision.append(_precision)\n",
    "        attribute_recall.append(_recall)\n",
    "        attribute_fscore.append(_fscore)\n",
    "\n",
    "    precision.append(attribute_precision)\n",
    "    recall.append(attribute_recall)\n",
    "    fscore.append(attribute_fscore)\n",
    "\n",
    "_, axes = plt.subplots(1, 3, figsize=(16 / 1.5, 9 / 4), gridspec_kw={\"wspace\": 0.3})\n",
    "for attribute_idx in sorted_idx:\n",
    "    attribute = attributes[attribute_idx]\n",
    "    attribute_thresholds = thresholds[attribute_idx]\n",
    "    attribute_precision = precision[attribute_idx]\n",
    "    attribute_recall = recall[attribute_idx]\n",
    "    attribute_fscore = fscore[attribute_idx]\n",
    "\n",
    "    ax = axes[0]\n",
    "    ax.plot(threshold, attribute_precision, label=attribute)\n",
    "\n",
    "    ax = axes[1]\n",
    "    ax.plot(threshold, attribute_recall, label=attribute)\n",
    "\n",
    "    ax = axes[2]\n",
    "    ax.plot(threshold, attribute_fscore, label=attribute)\n",
    "\n",
    "ax = axes[0]\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "\n",
    "ax = axes[1]\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Recall\")\n",
    "\n",
    "ax = axes[2]\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"F1 score\")\n",
    "plt.show()\n",
    "\n",
    "mean_f1 = np.mean(fscore, axis=0)\n",
    "_, ax = plt.subplots(figsize=(16 / 4, 9 / 4))\n",
    "ax.plot(threshold, mean_f1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 0.65\n",
    "threshold_idx = threshold.tolist().index(t)\n",
    "threshold_f1 = np.array(fscore)[:, threshold_idx]\n",
    "for attribute_idx, attribute in enumerate(attributes):\n",
    "    print(f\"{attribute}: {threshold_f1[attribute_idx]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
